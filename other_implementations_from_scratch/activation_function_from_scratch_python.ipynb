{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369c6210",
   "metadata": {},
   "source": [
    "## 1. Custom Sigmoid Module (like `nn.Sigmoid`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b25eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MySigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MySigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deefa10",
   "metadata": {},
   "source": [
    "### Example Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "act = MySigmoid()\n",
    "x = torch.tensor([0.0, 1.0, -2.0])\n",
    "y = act(x)  # tensor([0.5000, 0.7311, 0.1192])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e783d5",
   "metadata": {},
   "source": [
    "## 2. Custom Leaky ReLU Module (like `nn.LeakyReLU`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a7ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLeakyReLU(nn.Module):\n",
    "    def __init__(self, negative_slope=0.01):\n",
    "        super(MyLeakyReLU, self).__init__()\n",
    "        self.negative_slope = negative_slope\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(x >= 0, x, self.negative_slope * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994d005",
   "metadata": {},
   "source": [
    "### Example Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a41dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "act = MyLeakyReLU(negative_slope=0.01)\n",
    "x = torch.tensor([0.0, 1.0, -2.0])\n",
    "y = act(x)  # tensor([ 0.0000,  1.0000, -0.0200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d1266",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "\n",
    "* These classes behave just like PyTorch's `nn.Sigmoid()` and `nn.LeakyReLU()`.\n",
    "* You can insert them into your custom modelâ€™s `forward()` method like standard modules.\n",
    "* Backpropagation will work automatically because the operations use native `torch` functions (autograd handles derivatives)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
