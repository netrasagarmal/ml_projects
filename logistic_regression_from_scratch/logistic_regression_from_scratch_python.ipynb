{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d860252",
   "metadata": {},
   "source": [
    "### Step 1: Linear combination (same as linear regression)\n",
    "\n",
    "We first compute:\n",
    "\n",
    "$$\n",
    "z = m \\cdot x + b\n",
    "$$\n",
    "\n",
    "This is just a weighted sum of inputs â€” no difference from linear regression.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Sigmoid Activation\n",
    "\n",
    "We pass the output $z$ into a **sigmoid function** to squash it between 0 and 1:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "This gives us the **predicted probability** that the input belongs to class 1.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Binary Cross-Entropy Loss\n",
    "\n",
    "We use the **log loss** (a.k.a. binary cross-entropy) to measure prediction quality:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\cdot \\log(\\hat{y}_i) + (1 - y_i) \\cdot \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "Why?\n",
    "\n",
    "* It's ideal for probabilistic outputs\n",
    "* It penalizes confident wrong predictions more than less confident ones\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Gradient Descent to Update Parameters\n",
    "\n",
    "We compute gradients of the loss w\\.r.t. `m` and `b`, then update them:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial m} = \\frac{1}{n} \\sum ( \\hat{y}_i - y_i ) \\cdot x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial b} = \\frac{1}{n} \\sum ( \\hat{y}_i - y_i )\n",
    "$$\n",
    "\n",
    "Then update:\n",
    "\n",
    "$$\n",
    "m = m - \\alpha \\cdot \\frac{\\partial \\text{Loss}}{\\partial m}, \\quad b = b - \\alpha \\cdot \\frac{\\partial \\text{Loss}}{\\partial b}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba191fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa9f33e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Training Data ----------\n",
    "x_train = [0, 1, 2, 3, 4, 5]     # Feature (e.g., study hours)\n",
    "y_train = [0, 0, 0, 1, 1, 1]     # Labels (e.g., fail/pass)\n",
    "\n",
    "lst = [1, 2, 3, 4, 5]        \n",
    "x_train = [random.choice(lst) for _ in range(100)]        # Hours of study\n",
    "y_train = [random.choice([0,1]) for _ in range(100)]    # Test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a765fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Initialize Parameters ----------\n",
    "m = 0.0   # weight\n",
    "b = 0.0   # bias\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "n = len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa10ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Sigmoid Function ----------\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fb9cce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.6931, m = -0.0200, b = -0.0020\n",
      "\n",
      "Final model: sigmoid(-0.07x + 0.01)\n"
     ]
    }
   ],
   "source": [
    "# ---------- Training Loop ----------\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    dm = 0\n",
    "    db = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        z = m * x_train[i] + b\n",
    "        y_pred = sigmoid(z)\n",
    "\n",
    "        # Compute loss (just to monitor)\n",
    "        loss = - (y_train[i] * math.log(y_pred + 1e-8) + (1 - y_train[i]) * math.log(1 - y_pred + 1e-8))\n",
    "        total_loss += loss\n",
    "\n",
    "        # Compute gradients\n",
    "        error = y_pred - y_train[i]\n",
    "        dm += error * x_train[i]\n",
    "        db += error\n",
    "\n",
    "    # Average gradients and update parameters\n",
    "    m -= learning_rate * (dm / n)\n",
    "    b -= learning_rate * (db / n)\n",
    "\n",
    "    # Print every 100 steps\n",
    "    if epoch % 100 == 0:\n",
    "        avg_loss = total_loss / n\n",
    "        print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}, m = {m:.4f}, b = {b:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal model: sigmoid({m:.2f}x + {b:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea7d96",
   "metadata": {},
   "source": [
    "## 4. Why Each Step Matters\n",
    "\n",
    "| Step                  | What It Does                                        | Why It's Needed                           |\n",
    "| --------------------- | --------------------------------------------------- | ----------------------------------------- |\n",
    "| `sigmoid(z)`          | Converts raw score to a probability between 0 and 1 | Enables classification                    |\n",
    "| `log loss`            | Measures how wrong the prediction is                | Guides training to improve accuracy       |\n",
    "| `gradients`           | Tells us the direction to adjust `m` and `b`        | Minimizes the loss                        |\n",
    "| `learning_rate`       | Controls step size                                  | Prevents overshooting or slow convergence |\n",
    "| `looping over epochs` | Repeated updates                                    | Allows learning from data gradually       |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
