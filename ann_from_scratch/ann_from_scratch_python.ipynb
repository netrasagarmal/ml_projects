{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6200dfff",
   "metadata": {},
   "source": [
    "Let's walk through a **simple Artificial Neural Network (ANN)** from scratch in Python ‚Äî **no external libraries like PyTorch or TensorFlow** ‚Äî and explain each step clearly.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Goal: Build a Neural Network to Solve XOR or Binary Classification\n",
    "\n",
    "We'll use:\n",
    "\n",
    "* One input layer (2 features)\n",
    "* One hidden layer (2 neurons, ReLU or sigmoid)\n",
    "* One output layer (1 neuron, sigmoid activation)\n",
    "* Binary cross-entropy loss\n",
    "* Gradient descent to update weights\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Sample Dataset (Binary Classification)\n",
    "\n",
    "```python\n",
    "# XOR dataset: 2 inputs ‚Üí 1 output\n",
    "X = [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "]\n",
    "\n",
    "y = [0, 1, 1, 0]  # XOR output: only true when inputs differ\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Step-by-Step ANN Structure\n",
    "\n",
    "We‚Äôll create:\n",
    "\n",
    "* Input layer: 2 features\n",
    "* Hidden layer: 2 neurons\n",
    "* Output layer: 1 neuron\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Step 1: Define Activation Functions\n",
    "\n",
    "```python\n",
    "import math\n",
    "import random\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1 - sx)\n",
    "\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return 1 if x > 0 else 0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Step 2: Initialize Weights and Biases\n",
    "\n",
    "```python\n",
    "# Initialize random weights for input ‚Üí hidden layer (2x2)\n",
    "w1 = [[random.uniform(-1, 1) for _ in range(2)] for _ in range(2)]\n",
    "b1 = [random.uniform(-1, 1) for _ in range(2)]\n",
    "\n",
    "# Initialize weights for hidden ‚Üí output layer (2x1)\n",
    "w2 = [random.uniform(-1, 1) for _ in range(2)]\n",
    "b2 = random.uniform(-1, 1)\n",
    "\n",
    "learning_rate = 0.1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Step 3: Training Loop (Forward + Backward Pass)\n",
    "\n",
    "```python\n",
    "for epoch in range(10000):\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        # ---------- Forward Pass ----------\n",
    "        x0, x1 = X[i]\n",
    "        target = y[i]\n",
    "\n",
    "        # Input to hidden layer\n",
    "        z1 = [x0 * w1[0][0] + x1 * w1[0][1] + b1[0],\n",
    "              x0 * w1[1][0] + x1 * w1[1][1] + b1[1]]\n",
    "        a1 = [sigmoid(z1[0]), sigmoid(z1[1])]\n",
    "\n",
    "        # Hidden to output layer\n",
    "        z2 = a1[0] * w2[0] + a1[1] * w2[1] + b2\n",
    "        output = sigmoid(z2)\n",
    "\n",
    "        # ---------- Loss ----------\n",
    "        loss = -(target * math.log(output + 1e-8) + (1 - target) * math.log(1 - output + 1e-8))\n",
    "        total_loss += loss\n",
    "\n",
    "        # ---------- Backward Pass ----------\n",
    "        d_loss_output = output - target\n",
    "        d_output_z2 = sigmoid_derivative(z2)\n",
    "\n",
    "        # Gradients for w2 and b2\n",
    "        d_w2 = [d_loss_output * d_output_z2 * a for a in a1]\n",
    "        d_b2 = d_loss_output * d_output_z2\n",
    "\n",
    "        # Backprop to hidden layer\n",
    "        d_hidden = [d_loss_output * d_output_z2 * w2[j] * sigmoid_derivative(z1[j]) for j in range(2)]\n",
    "\n",
    "        # Gradients for w1 and b1\n",
    "        d_w1 = [[d_hidden[j] * x for x in [x0, x1]] for j in range(2)]\n",
    "        d_b1 = d_hidden\n",
    "\n",
    "        # ---------- Update Weights ----------\n",
    "        for j in range(2):\n",
    "            for k in range(2):\n",
    "                w1[j][k] -= learning_rate * d_w1[j][k]\n",
    "            b1[j] -= learning_rate * d_b1[j]\n",
    "\n",
    "        for j in range(2):\n",
    "            w2[j] -= learning_rate * d_w2[j]\n",
    "        b2 -= learning_rate * d_b2\n",
    "\n",
    "    # Print every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss = {total_loss:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Step 4: Test the Network\n",
    "\n",
    "```python\n",
    "print(\"\\n--- Testing ---\")\n",
    "for i in range(len(X)):\n",
    "    x0, x1 = X[i]\n",
    "\n",
    "    z1 = [x0 * w1[0][0] + x1 * w1[0][1] + b1[0],\n",
    "          x0 * w1[1][0] + x1 * w1[1][1] + b1[1]]\n",
    "    a1 = [sigmoid(z1[0]), sigmoid(z1[1])]\n",
    "\n",
    "    z2 = a1[0] * w2[0] + a1[1] * w2[1] + b2\n",
    "    output = sigmoid(z2)\n",
    "\n",
    "    print(f\"Input: {x0, x1} => Output: {output:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Explanation of Steps\n",
    "\n",
    "| Step                 | Explanation                                                          |\n",
    "| -------------------- | -------------------------------------------------------------------- |\n",
    "| **Weight Init**      | We randomly initialize weights for symmetry breaking                 |\n",
    "| **Forward Pass**     | We compute activations through the network using sigmoid             |\n",
    "| **Loss Function**    | We use binary cross-entropy to measure how far output is from target |\n",
    "| **Backward Pass**    | Use chain rule to compute how much each weight contributed to error  |\n",
    "| **Gradient Descent** | We subtract gradient times learning rate from each weight            |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why This Works\n",
    "\n",
    "* The network can **learn non-linear boundaries** (like XOR)\n",
    "* **Sigmoid** squashes outputs to probability range\n",
    "* **Backpropagation** updates weights by calculating how much each one affects final error\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a **visual of how the network classifies** the input space?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577adf51",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
